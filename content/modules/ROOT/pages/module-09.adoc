= Creating an install iso with Anaconda

In this lab you will learn the basics of deploying a bootc image using Anaconda and an
all in one `iso` image.

Anaconda is the official Red Hat Enterprise Linux installer and it uses Kickstart as it's scripting language.
Recently, Kickstart received a new command called `ostreecontainer`.

[#build]
== Building an installation ISO image with the bootc container

Anaconda is typically used via the boot iso shipped by Red Hat. There are several ways to get a host to boot from this ISO 
image including PXE or HTTP Boot over the network, as an image via IMPI, or a phyiscal disk inserted into a drive. The kickstart 
file for an install can also be provided in several ways, including as part of the iso or over a network. For this lab, 
we'll focus on creating a boot ISO that includes the kickstart, mainly for local logistics reasons. The configuration of 
Anaconda via kickstart will be the same, regardless of how they are presented to the host being installed.

The starting iso image should have been already downloaded at the setup phase at the beginning of this workshop. You can check it exists
by listing the file `/var/lib/libvirt/images/summit/rhel-boot.iso` as root:

[source,bash]
----
sudo ls -lah /var/lib/libvirt/images/summit/rhel-boot.iso
----

If it is missing, you can redownload it by calling `make iso-download`.

Next, make sure you have pushed the bootc container image to the local registry:

----
podman push summit.registry/lb1506:bootc
----

The actual generation of the custom `iso` image is out of scope for this workshop, so we are providing a make command to do so. This 
custom ISO will contain not the original boot image, but our kickstart file and the bootc image we want installed on the host. These 
sort of all in one installers can be very useful for bare metal, especially in places with limited or no network connectivity.

[source,bash]
----
make iso CONTAINER=summit.registry/lb1506:bootc
----

At the end of the run, you should be able to list the file `/var/lib/libvirt/images/summit/rhel-boot-custom.iso` as root:

----
sudo ls -lah /var/lib/libvirt/images/summit/rhel-boot-custom.iso
----

*For reference*, the script used to embed and build the custom ISO can be found in `bin/embed-container` and it can
be replicated with the following steps (*no need to run these commands*):

  1. use `skopeo` to copy the container image to a temporary directory on disk:

+
----
skopeo copy "docker://summit.registry/lb1506:bootc" "oci:${TEMPDIR}/container/"
----

  2. generate a Kickstart file (more details below)
  3. create the new iso using `mkksiso`

+
----
mkksiso --ks local.kickstart --add ${TEMPDIR}/container/ original.iso custom.iso
----

[#kickstart]
== Exploring the kickstart file

The Kickstart added to the iso is in `config/local.ks` and looks like this.

----
text
network --bootproto=dhcp --device=link --activate
clearpart --all --initlabel --disklabel=gpt
reqpart --add-boot
part / --grow --fstype xfs

ostreecontainer --url=/run/install/repo/container --transport=oci

firewall --disabled
services --enabled=sshd
sshkey --username lab-user "SSHKEY"
user --name="lab-user" --groups=wheel --plaintext --password=lb1506
rootpw lb1506
poweroff
----

The new `ostreecontainer` command replaces the standard %packages block of the kickstart file that defines what RPMs would be installed by Anaconda. 
This replaces that set of RPM transactions with the `bootc` image to be unpacked and installed to disk.  

  * `--url=/run/install/repo/container` -> path to the local container in the iso but it can also directly take a publicly available registry
  * `--transport=oci` -> the format in which the container is available, in this case `oci`

If you wanted to deploy directly from a container registry, the `ostreecontainer` command would look like this (*no need to run this command*):

----
ostreecontainer --url=quay.io/myorg/myimage:mytag
----

All of the rest of the kickstart is standard fare. Anaconda will create the partitions and filesystems on disk, enable services, and create users 
just like it would in any other install. You even have access to the `%pre` and `%post` blocks too, the only missing section from any kickstart you 
have today is the `%packages` list.

Once you see the following output, you can proceed.
....
ISO image produced: 1028128 sectors
Written to medium : 1028288 sectors at LBA 32
Writing to '/var/lib/libvirt/images/summit/rhel-boot-custom.iso' completed successfully.

Done
....

[#run]
== Starting the installation with Anaconda in a virtual machine

Having created a custom installation ISO, we can boot a virtual machine and automatically install our image. Creating
a virtual machine from scratch is out of scope for this workshop, so we have provided a make command. We are once again using 
`virt-install` to create a local VM, the main difference is that here we are using the `--location` option to pass the local 
path of the ISO file to the install process. This emulates inserting a phyiscal disk into a bare metal system.

----
make vm-iso
----

The command above will start the installation process and you should see the console of a newly created virtual machine
booting from the custom iso. Watch for the Anaconda output as it proceeds with the unattended install. 

....
Powering off.
[   65.973246] reboot: Power down

Domain creation completed.
You can restart your domain by running:
  virsh --connect qemu:///system start iso
virsh --connect "qemu:///system" start "iso"
Domain 'iso' started

....

You can now see if the virtual machine is running:

[source,bash]
----
virsh --connect qemu:///system list
----
....
 Id   Name   State
----------------------
 1    qcow   running
 4    iso    running
....

[#test]
== Test and login to the virtual machine

Like with the previous virtual machine created, you can directly see if the http application is already running on the host:

[source,bash]
----
curl http://iso-vm
----

The output should be "Hello Red Hat Summit Connect 2024!!"

You should also be able to login to the virtual machine:

----
ssh lab-user@iso-vm
----

If the ssh key is not automatically picked up, use the password `lb1506`.

You can now check the status of `bootc`:

----
sudo bootc status
----

The output should be similar to this:

[source,yaml]
----
apiVersion: org.containers.bootc/v1alpha1
kind: BootcHost
metadata:
  name: host
spec:
  image:
    image: /run/install/repo/container
    transport: oci
  bootOrder: default
status:
  staged: null
  booted:
    image:
      image:
        image: /run/install/repo/container
        transport: oci
      version: 9.20240501.0
      timestamp: null
      imageDigest: sha256:0a3daed6e31c2f2917e17ea994059e1aaee0481fe16836c118c5e1d10a87365c
    cachedUpdate: null
    incompatible: false
    pinned: false
    ostree:
      checksum: 42f36e87a9436d505b3993822b92dbf7961ad3f1a8fddf67b91746df365784f0
      deploySerial: 0
  rollback: null
  rollbackQueued: false
  type: bootcHost
----

[#switch]
== Switching to a different transport method

One thing that immediately is different in the `bootc status` output is that the deployed image image is a local path, not the 
container naming convention we've been using:

[source,yaml]
----
spec:
  image:
    image: /run/install/repo/container
    transport: oci
  bootOrder: default
----

The `transport` line refers to the OCI definition of images which includes how they are pulled. The `oci` transport means 
this is a single image located at a specific local path. This is useful for installing the way we did, but less so for updates. 

So far in this lab, we have been using the `registry` transport, which requires network access. If we wanted to manage updates in an offline manner, 
say for disconnected environments or those with intermittent connectivity, we can use `containers-storage` which refers to 
the locally configured shared locations. A full discussion of transports and their associated uses and configuration is outside 
the scope of this lab.

To keep with the theme of offline usage, let's simulate updating from the local storage. We can use `skopeo` to copy images from one location to another. 
In the embedded ISO script, it's used to copy from the registry to be included in the final image. Here, we can use it to copy from 
the registry to the host. We need to be sure to use `sudo` to copy into the system storage location and not the user's.

[source,bash]
----
sudo skopeo copy --tls-verify=false docker://summit.registry/lb1506:bootc  containers-storage:summit.registry/lb1506:bootc 
----

Switch our installation to use the new container image, using the `--transport` flag to let bootc know we want to use local 
container storage for this operation.

[source,bash]
----
sudo bootc switch --transport containers-storage summit.registry/lb1506:bootc
----

....
Loading usr/lib/ostree/prepare-root.conf
Queued for next boot: ostree-unverified-image:containers-storage:summit.registry/lb1506:bootc
  Version: 9.20240501.0
  Digest: sha256:0a3daed6e31c2f2917e17ea994059e1aaee0481fe16836c118c5e1d10a87365c
....

At this point, the "new" installation has been prepared and will be started at next boot of the virtual machine.

The last step for the change to take is to reboot the virtual machine. Before doing it, please make sure you are logged in to the
virtual machine and not the hypervisor (the prompt should look like `[lab-user@lb1506-vm ~]$`):

[source,bash]
----
sudo systemctl reboot
----

In a short time after that command, you should be able to ssh back to the virtual machine:

[source,bash]
----
ssh lab-user@iso-vm
----

And check the bootc status:

[source,bash]
----
sudo bootc status
----

[source,yaml]
----
apiVersion: org.containers.bootc/v1alpha1
kind: BootcHost
metadata:
  name: host
spec:
  image:
    image: summit.registry/lb1506:bootc
    transport: containers-storage
  bootOrder: default
status:
  staged: null
  booted:
    image:
      image:
        image: summit.registry/lb1506:bootc
        transport: containers-storage
      version: 9.20240501.0
      timestamp: null
      imageDigest: sha256:0a3daed6e31c2f2917e17ea994059e1aaee0481fe16836c118c5e1d10a87365c
    cachedUpdate: null
    incompatible: false
    pinned: false
    ostree:
      checksum: 6e468a048b5c86ed8c481040b125b442b9222c914fc12799123717eb94fc43b6
      deploySerial: 0
  rollback:
    image:
      image:
        image: /run/install/repo/container
        transport: oci
      version: 9.20240501.0
      timestamp: null
      imageDigest: sha256:0a3daed6e31c2f2917e17ea994059e1aaee0481fe16836c118c5e1d10a87365c
    cachedUpdate: null
    incompatible: false
    pinned: false
    ostree:
      checksum: 42f36e87a9436d505b3993822b92dbf7961ad3f1a8fddf67b91746df365784f0
      deploySerial: 0
  rollbackQueued: false
  type: bootcHost
----

In the status you can see `bootc` is now tracking local container storage for updates. Further updates would then be provided on media 
presented to the host, like a USB drive or DVD. You could use skopeo sync a registry repository to media as well as copy it 
from the media to the local storage on the host. These images are visible to podman as well. 

[source,bash]
----
sudo podman images
----

There are a range of possibilities for edge devices, disconnected networks, and any other arenas where direct connectivity to a 
registry over a network isn't possible or desired. 

[#complete]
== Workshop complete!
You've now completed all of the exercises in today's workshop.

Image mode for RHEL provides a new way to think about risks involved with updating hosts, creates native rollback functionality, 
and can quickly and easily change the role of a particular host. We hope you've had a few ideas of how the techiniques and topics 
in this workshop could apply to the environments you manage.
